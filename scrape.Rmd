---
title: "Scraping Websites"
output: html_document
---

The libraries currently in use in scrape.R are as follows:

```{r, message = FALSE}
library(tidyverse)
library(rvest)
library(tidytext)
library(RCurl)
source("R/get_data.R")
```

I have added `tidytext` for manipulation of the text on the resulting webpages. `RCurl` contains a useful function to check if a url exists, as, previously, some of the results would lead to 404 errors.

```{r, include = FALSE, cache = TRUE}
SEARCH_PREFIX <- "http://www.bing.com/search?q="
SEARCH_TERMS  <- "covid coronavirus spring 2020 grading policy undergraduate academic" # review scrape_cht.R if more advanced operators are required. 

# from R/get_data.R. Retrieves the data set from the web and scores it in the data/ subdirectory, or reads it from that directory if it already exists. 
schools <- read_scorecard() 

# basic filtering. Replicates CHT's subset commands

schools <- schools %>% 
	filter(CCBASIC >= 15,
				 !(CCUGPROF %in% 0:4),
				 HIGHDEG  %in% 3:4,       
				 CCSIZSET %in% 6:17,    # Eliminate graduate-only institutions
				 !(CCBASIC  %in% 24:32), # Eliminate special focus schools
				 !(CONTROL==3),         # Eliminate for-profit schools
				 CURROPER==1) 			    # Eliminate institutions that are no longer operating

# clean out columns we don't need anymore
schools <- schools %>% 
	select(INSTNM, URL = INSTURL)

# construct urls + domains for different institutions
schools <- schools %>% 
	mutate(URL      = tolower(URL),
				 URL      = str_replace_all(URL, ".edu.*",".edu"),
				 URL      = str_replace_all(URL, "/$",""),
				 has_http = grepl("^http", URL),
				 URL      = ifelse(has_http, URL, paste0("http://", URL)),
				 domain   = map_chr(URL, ~str_split(.,"https*://")[[1]][[2]]),
				 domain   = str_replace_all(domain, "w+\\.|w+[1-9]\\.","")
	) %>% 
	select(-has_http)

# construct search URLS

schools <- schools %>% 
	mutate(search = SEARCH_PREFIX,
				 search = paste0(search, "site:", domain, "+"),
				 search = paste0(search, str_replace_all(SEARCH_TERMS, " ", "+")), 
				 search = map_chr(search, URLencode))

# Let's test gathering some links
get_page <- function(url){
	url <- url %>% 
		read_html() %>% 
		html_nodes("h2 a") %>% # All hyperlinks immediately in an h2 tag are results
		xml_attr("href") # Grab only the link
	
	Sys.sleep(3)
	
	return
}
```


## Checking URLs

The first task addressed was comparing the code's results to the confirmed webpages we have from Team Gibbons. We want to figure out how frequently the webscraper actually picks up the correct pages. The following code collects the info from the sheet of confirmed urls, filters out any schools without confirmed urls, and transforms the school's url to match a uniform pattern.

```{r, message = FALSE, results = 'hide'}
grade_urls <- read_grade_urls() %>%
	filter(!is.na(grades_url)) %>%
	mutate(url      = tolower(url), #Get matching urls
				 url      = str_replace_all(url, ".edu.*",".edu"),
				 url      = str_replace_all(url, "/$",""),
				 has_http = grepl("^http", url),
				 url      = ifelse(has_http, url, paste0("http://", url)),
				 domain   = map_chr(url, ~str_split(.,"https*://")[[1]][[2]]),
				 domain   = str_replace_all(domain, "w+\\.|w+[1-9]\\.","")
	) %>% 
	select(-has_http)
```

Creating uniform urls allows us to compare the urls in Team Gibbons's spreadsheet and the scorecard. I then filtered out schools from the scorecard that so we have a list of schools that match the ones we have confirmed urls for.

```{r}
school_urls <- filter(schools, INSTNM %in% grade_urls$institution, domain %in% grade_urls$domain)
```

```{r, include = FALSE}
num_schools <- nrow(school_urls)
```

Using this new list, we can use the `get_page` function to automatically complete the Bing searches for each school, collect all the results from the first page, and check to make sure each collected url exists.

```{r, cache = TRUE}
school_urls <- school_urls %>%
	mutate(hits = map(search, get_page)) %>% # Complete search
	unnest(c(hits)) %>% # Make each hit a unique item in the table
	mutate(exists = map(hits, url.exists)) %>% # Check which urls exist
	filter(exists == TRUE) # Keep only results that do not yield 404 errors
```

And, now that we have a list of hits from our scrape algorithm and a list of the correct pages from Team Gibbons, we can compare the results to see how many schools give the correct result on the first page.

```{r}
grade_urls_in_bing <- filter(grade_urls, grades_url %in% school_urls$hits)
```

Unfortunately, only `r nrow(grade_urls_in_bing)` out of a possible `r num_schools` correct urls were found on the first page of Bing results. This means that there needs to be some kind of tweaking to the search or we'll have to grab more than just the first page of results from Bing. It's also possible that some schools have their grading policy on multiple pages, and the webscraper grabs a correct page, but not the same one Team Gibbons found. This could be worth exploring further.

Another thing to keep in mind here is that Bing tailors its search results to each user. Therefore, each search can yield different results. In testing, I saw the number of correct urls found by the scraper decrease from 25 to 24. And, in the next section, I found that the correct url changed position in the Bing search between days, which means the results, while relatively consistent, are not always the same.

## Ranking Results by Text Mining

The next step was ranking the results in hopes that volunteers would not have to sift through an entire page of Bing search results to find the correct page. Instead, we hope to create a ranking system that accurately determines which results are most likely to have the grading policy for the Spring 2020 semester.

```{r, include = FALSE}
school_order <- school_urls %>%
	select(INSTNM, domain, hits) %>%
	group_by(domain) %>%
	mutate(rank = row_number(INSTNM))

school_order_in_bing <- filter(school_order, hits %in% grade_urls$grades_url)
```

For reference, if the rank were the order the results show up in, the average rank of the correct page would be `r mean(school_order_in_bing$rank)` and the median would be `r median(school_order_in_bing$rank)`. These are obviously quite good results, but I wanted to see if they could be improved.

The first method is a simple sum of keywords found on each of the pages. The keywords I used were simply the search terms Team Gibbons found most effective. While these are likely not the best keywords to look for, I thought they would work for some preliminary testing. We may see better results if we work on finding a different set of keywords to count.

```{r, message = FALSE, results = 'hide', include = FALSE}
data(stop_words)
keywords <- strsplit(SEARCH_TERMS, " ")

get_ksum <- function(url) {
	print(url)
	
	text <- url %>%
		read_html() %>%
		html_text() %>%
		str_replace_all("\n", " ")
	
	text_df <- tibble(text = text) %>%
		unnest_tokens(word, text) %>%
		anti_join(stop_words) %>%
		count(word, sort = TRUE)
	
	# Counting occurrences of each word in keywords
	counts <- lapply(keywords, function(x) {
		index <- match(x, text_df[[1]])
		return(text_df[[2]][index])
	}) %>%
		unlist()
	
	return(sum(counts, na.rm = TRUE))
}
```

For this, I created a function `get_ksum` which, given a url, sums the occurrences of the keywords and returns that number. This function is then mapped to the school list as a new variable so each hits has a number of keywords associated with it.

```{r, message = FALSE, results = 'hide', cache = TRUE}
school_urls <- school_urls %>%
	select(INSTNM, domain, hits) %>%
	mutate(keyword_sum = unlist(map(hits, get_ksum)))
```

From there, the schools are ranked based on which has the highest number of keyword occurrences. A lower rank means it is more likely to be the correct page. Each school is ranked seperately from others, so each school will have a rank 1 hit which will hopefully contain the desired information.

```{r}
school_urls <- school_urls %>%
	group_by(domain) %>%
	mutate(rank = rank(desc(keyword_sum), ties.method = "first"))

# Sort by rank
school_urls <- arrange(school_urls, INSTNM, rank)
```

The last bit of data I collected was a list of the hits and what the ranks of the correct urls were.

```{r}
school_urls_in_bing <- filter(school_urls, hits %in% grade_urls$grades_url)
```

This is a dataset containing only the schools that got the correct result in the first page of Bing. On average, the rank of the correct page was `r mean(school_urls_in_bing$rank)`, and the median of the ranks was `r median(school_urls_in_bing$rank)`.

```{r, include = FALSE, cache = TRUE}
get_kratio <- function(url) {
	print(url)

	text <- url %>%
		read_html() %>%
		html_text() %>%
		str_replace_all("\n", " ")

	text_df <- tibble(text = text) %>%
		unnest_tokens(word, text) %>%
		anti_join(stop_words) %>%
		count(word, sort = TRUE)

	total <- sum(text_df$n)

	# Counting occurrences of each word in keywords
	counts <- lapply(keywords, function(x) {
		index <- match(x, text_df[[1]])
		return(text_df[[2]][index])
	}) %>%
		unlist()

	return(sum(counts, na.rm = TRUE) / total)
}

# Get keyword ratios
school_urls <- school_urls %>%
	select(INSTNM, domain, hits) %>%
	mutate(keyword_rat = unlist(map(hits, get_kratio)))

# Rank by group where group is school (group by domain because school names are not unique)
school_urls <- school_urls %>%
	group_by(domain) %>%
	mutate(rank = rank(desc(keyword_rat), ties.method = "first"))

# Sort by rank
school_urls <- arrange(school_urls, INSTNM, rank)

# With ranks displayed
school_urls_in_bing <- filter(school_urls, hits %in% grade_urls$grades_url)
```


I also tried ranking by ratio of keywords to non-keywords by page, but the results averaged a lower rank for the correct page. For this method, the rank of the correct page averaged `r mean(school_urls_in_bing$rank)` and had a median of `r median(school_urls_in_bing$rank)`.

As you can see, these results of keyword ranking are not as effective as the order the results show up in the Bing search. Better choice of keywords could hypothetically improve the results, but I find it unlikely. The amount of time it would take to find keywords that improve the ranking would probably be better spent elsewhere, as the correct pagers tend to show up in the first three results anyway. In the following table of all the correct urls found, the rank is the position of the link in the Bing search results for the correct pages.

```{r echo = FALSE, results = 'asis'}
library(knitr)
kable(school_order_in_bing)
```

## Next Steps

Currently, we are finding the correct result on the first page approximately `r round(nrow(grade_urls_in_bing)/num_schools * 100, 2)` percent of the time. For those that are found, they tend to be near the first result, which is where we want to be. It's unlikely we can improve on this result by another ranking system.

The only issue we really face here is the amount of schools we're finding the correct result for. It's possible, as mentioned earlier, that some of the other ones find a correct page that's different than the one Team Gibbons found, but it's unlikely that is true for all the schools we miss. It is possible that going to the second page of Bing results will hit more correct pages, but they will be ranked much further down and thus make the crowdsourcers' job much more tedious.

Possible solutions include some sort of boolean operators in the search query (such as requiring certain words) or slight tweaking of the keywords we search for.