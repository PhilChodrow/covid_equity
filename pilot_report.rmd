---
title: "Pilot Study Report"
author: "Elisabeth, Josef, and Phil"
date: "7/29/2020"
output:
  html_document:
    theme: united
    toc: true
    toc_depth: 1
    toc_float: true
    number_sections: true
---

# Purpose

In this report, we will study the quality of the answers we received from Amazon Mechanical Turk crowdsourcers in our pilot study. Our primary aims are to: 

1. Assess the **quality** of the answers we received. If we ran exactly the same methodology on the full list of institutions, how much reliable data would we gather? 
2. **Make recommendations** for improving the quality of the data we gather in the large-scale study. 

# Approach {.tabset}

## Data Completeness

A basic question: how often do we have any data at all? Possible metrics: 

1. How many `NA`s do we have?
2. For how many institutions do we have at least one answer in each question?

## Presence of Consensus

If we do have data, how often is there a clear consensus on the answer? Some possible metrics: 

1. In which schools and questions is there full consensus among the workers?
2. In which schools and questions is there a clear majority answer? 

## Reliability of Answers

We've noticed at least one case in which it appears that a worker did not even attempt to retrieve the information, and instead just clicked through the form. Can we detect this kind of behavior and discourage it? Or, at least, can we discount those answers in the analysis phase? 
This is a future question we might take up -- does not need to be pursued in this early report. Some possible diagnostics: 

1. Answer-level consistency checks. Did a worker give contradictory answers to two questions for the same school?
2. Worker-level assessments. Worker rating; average time spent on task; comparing answers across multiple schools. 

# Analysis

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(knitr)

df <- read_csv("data/Batch_4133302_batch_results.csv")
```

Here are a few things that I looked at very briefly -- feel free to modify, discard, or use. 

The data has ```r nrow(df)``` rows. 

```{r}
sub <- df %>% 
	select(LifetimeApprovalRate, AcceptTime, SubmitTime, Input.INSTNM,  contains("Answer"))
```

```{r, warning=FALSE, message=FALSE}

completeness <- sub %>% 
	group_by(Input.INSTNM) %>% 
	summarise(across(contains("Answer"), ~mean(!is.na(.x)))) %>% 
	summarise(across(contains("Answer"), mean)) %>% 
	pivot_longer(contains("Answer")) %>% 
	rename(`% Complete` = value)

num_answers <- sub %>% 
	group_by(Input.INSTNM) %>% 
	summarise(across(contains("Answer"), 
									 ~length(unique(na.omit(.x))))) %>% 
	summarise(across(contains("Answer"), mean)) %>% 
	pivot_longer(contains("Answer")) %>% 
	rename(`Mean # Distinct Answers` = value)

completeness %>% 
	left_join(num_answers, by = c("name" = "name")) %>% 
	rename(Question = name) %>% 
	arrange(desc(`% Complete`)) %>% 
	kable(digits = c(0, 2, 1))
```

## Majority

How often is there a majority answer to a given question? In this quick analysis, we define "majority" as: 

1. At least two workers agree on a question. 
2. Any other workers have picked different answer: there is no other answer with two workers endorsing it. 

```{r, warning=FALSE, message=FALSE}

df %>% 
	select(Input.INSTNM, contains("Answer")) %>% 
	mutate(across(contains("Answer"), as.character)) %>% 
	pivot_longer(contains("Answer")) %>% 
	filter(!is.na(value), value != "cannotdetermine") %>%  # treating NA and "cannotdetermine" as the same
	group_by(Input.INSTNM, name, value) %>% 
	summarise(n = n()) %>% 
	mutate(majority_exists = (max(n) > 1) &  ((max(n) / sum(n)) > .5)) %>% 
	summarise(majority_exists = max(majority_exists)) %>% 
	ungroup() %>% 
	group_by(name) %>%
	summarise(`% Majority Exists` = sum(majority_exists)/length(unique(df$Input.INSTNM))) %>% 
	arrange(desc(`% Majority Exists`)) %>% 
	kable(digits = 2)

```



(Elisabeth) Mostly I'm just trying to get a feel for working with R here, but I've created a snippet of code to explore Chad's idea of finding suspicious workers based on work time. I set the minimum time by choosing the percent, and then the code compiles a list of workers who completed the survey in less than the minimum time.

```{r, warning=FALSE, message=FALSE}
min_time <- quantile(df$WorkTimeInSeconds, 0.25)
sus_workers <- c()

for(i in 1:nrow(df))
  {if(df$WorkTimeInSeconds[i] < min_time)
  {sus_workers[i] <- df$WorkerId[i]}}

unique(sus_workers)
```

(Phil): Elisabeth, this is great! I see you are already effectively navigating many of `R`'s weirdnesses, like "`<-`", and "`c()`", as well as some of my favorite functions like `quantile` and `unique`.  

- **Follow-up questions**: How many total answers would we lose if we discarded the work of suspicious workers? Also, is there a more systematic way to choose the quantile? 
- The code you wrote makes perfect sense to me. If you're interested, there are ways to make it more "idiomatic" for `R`. In particular, many functions in `R` are *vectorized* (work on an entire dataframe column at once), and so you usually don't need for-loops over rows. Here's exactly the same logic in slightly more `R`-like code. Both solutions are correct -- whether or not to follow up on the "idiomatic" ways is entirely up to you and probably will not impact work on this project.   

```{r, warning = FALSE, message = FALSE}

# x %>% f(y) is the same as f(x,y)
df %>% 
	mutate(sus = WorkTimeInSeconds < min_time) %>% # boolean column for suspicious workers
	filter(sus) %>%                                # keep only those rows for which sus = TRUE
	distinct(WorkerId) %>%                         # show each distinct ID once
	kable()                                        # print it all pretty-like
```
# Discussion and Recommendations