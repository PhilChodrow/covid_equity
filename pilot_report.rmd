---
title: "Pilot Study Report"
author: "Elisabeth, Josef, and Phil"
date: "7/29/2020"
output:
  html_document:
    theme: united
    toc: true
    toc_depth: 1
    toc_float: true
    number_sections: true
---

# Purpose

In this report, we will study the quality of the answers we received from Amazon Mechanical Turk crowdsourcers in our pilot study. Our primary aims are to: 

1. Assess the **quality** of the answers we received. If we ran exactly the same methodology on the full list of institutions, how much reliable data would we gather? 
2. **Make recommendations** for improving the quality of the data we gather in the large-scale study. 

# Approach {.tabset}

## Data Completeness

A basic question: how often do we have any data at all? Possible metrics: 

1. How many `NA`s do we have?
2. For how many institutions do we have at least one answer in each question?

## Presence of Consensus

If we do have data, how often is there a clear consensus on the answer? Some possible metrics: 

1. In which schools and questions is there full consensus among the workers?
2. In which schools and questions is there a clear majority answer? 

## Reliability of Answers

We've noticed at least one case in which it appears that a worker did not even attempt to retrieve the information, and instead just clicked through the form. Can we detect this kind of behavior and discourage it? Or, at least, can we discount those answers in the analysis phase? 
This is a future question we might take up -- does not need to be pursued in this early report. Some possible diagnostics: 

1. Answer-level consistency checks. Did a worker give contradictory answers to two questions for the same school?
2. Worker-level assessments. Worker rating; average time spent on task; comparing answers across multiple schools. 

# Analysis

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(knitr)

df <- read_csv("data/Batch_4133302_batch_results.csv")
```

Here are a few things that I looked at very briefly -- feel free to modify, discard, or use. 

The data has ```r nrow(df)``` rows. 

```{r}
sub <- df %>% 
	select(LifetimeApprovalRate, AcceptTime, SubmitTime, Input.INSTNM,  contains("Answer"))
```

```{r, warning=FALSE, message=FALSE}

completeness <- sub %>% 
	group_by(Input.INSTNM) %>% 
	summarise(across(contains("Answer"), ~mean(!is.na(.x)))) %>% 
	summarise(across(contains("Answer"), mean)) %>% 
	pivot_longer(contains("Answer")) %>% 
	rename(`% Complete` = value)

num_answers <- sub %>% 
	group_by(Input.INSTNM) %>% 
	summarise(across(contains("Answer"), 
									 ~length(unique(na.omit(.x))))) %>% 
	summarise(across(contains("Answer"), mean)) %>% 
	pivot_longer(contains("Answer")) %>% 
	rename(`Mean # Distinct Answers` = value)

completeness %>% 
	left_join(num_answers, by = c("name" = "name")) %>% 
	rename(Question = name) %>% 
	arrange(desc(`% Complete`)) %>% 
	kable(digits = c(0, 2, 1))
```

## Majority

How often is there a majority answer to a given question? In this quick analysis, we define "majority" as: 

1. At least two workers agree on a question. 
2. Any other workers have picked different answer: there is no other answer with two workers endorsing it. 

```{r, warning=FALSE, message=FALSE}

df %>% 
	select(Input.INSTNM, contains("Answer")) %>% 
	mutate(across(contains("Answer"), as.character)) %>% 
	pivot_longer(contains("Answer")) %>% 
	filter(!is.na(value), value != "cannotdetermine") %>%  # treating NA and "cannotdetermine" as the same
	group_by(Input.INSTNM, name, value) %>% 
	summarise(n = n()) %>% 
	mutate(majority_exists = (max(n) > 1) &  ((max(n) / sum(n)) > .5)) %>% 
	summarise(majority_exists = max(majority_exists)) %>% 
	ungroup() %>% 
	group_by(name) %>%
	summarise(`% Majority Exists` = sum(majority_exists)/length(unique(df$Input.INSTNM))) %>% 
	arrange(desc(`% Majority Exists`)) %>% 
	kable(digits = 2)
```

```{r}
df %>% 
	ggplot() + 
	aes(x = WorkTimeInSeconds) + 
	geom_histogram(bins = 100)
```

## Control Flow

```{r}
num_conflicts <- df %>% 
	group_by(Input.INSTNM, WorkerId) %>% 
	summarise(conflict = sum((Answer.universal == "yes")*(!is.na(Answer.optinpolicy)))) %>% 
	summarise(conflict = sum(conflict)) %>% 
	summarise(num_conflicts = sum(conflict, na.rm = T))

```

There were `r num_conflicts$num_conflicts[1]` instances in which workers answered the the policy was "universal" but proceeded to answer the opt-in policy question as well. 



(Elisabeth) Mostly I'm just trying to get a feel for working with R here, but I've created a snippet of code to explore Chad's idea of finding suspicious workers based on work time. I set the minimum time by choosing the percent, and then the code compiles a list of workers who completed the survey in less than the minimum time.

```{r, warning=FALSE, message=FALSE}
min_time <- quantile(df$WorkTimeInSeconds, 0.25)
sus_workers <- c()

for(i in 1:nrow(df))
  {if(df$WorkTimeInSeconds[i] < min_time)
  {sus_workers[i] <- df$WorkerId[i]}}

unique(sus_workers)
```

(Phil): Elisabeth, this is great! I see you are already effectively navigating many of `R`'s weirdnesses, like "`<-`", and "`c()`", as well as some of my favorite functions like `quantile` and `unique`.  

- **Follow-up questions**: How many total answers would we lose if we discarded the work of suspicious workers? Also, is there a more systematic way to choose the quantile? 
- The code you wrote makes perfect sense to me. If you're interested, there are ways to make it more "idiomatic" for `R`. In particular, many functions in `R` are *vectorized* (work on an entire dataframe column at once), and so you usually don't need for-loops over rows. Here's exactly the same logic in slightly more `R`-like code. Both solutions are correct -- whether or not to follow up on the "idiomatic" ways is entirely up to you and probably will not impact work on this project.   

```{r, warning = FALSE, message = FALSE}

# x %>% f(y) is the same as f(x,y)
df %>% 
	mutate(sus = WorkTimeInSeconds < min_time) %>% # boolean column for suspicious workers
	filter(sus) %>%                                # keep only those rows for which sus = TRUE
	distinct(WorkerId) %>%                         # show each distinct ID once
	kable()                                        # print it all pretty-like
```




## Logical Flow

Could and did the workers follow the logical flow of the questions? The logical flow was based on whether or not there was a universal policy, so we have two cases.

1. The worker marked "yes" for universal. In this case, the worker did not follow the logical flow if they answered any of the questions besides what the universal policy was.
2. The worker marked "no" for universal. In this case, the worker did not follow the logical flow if they left blank any of the questions besides universal policy or if they answered universal policy.

```{r, warning=FALSE, message=FALSE, include=FALSE}
# All points for which universal was marked yes
# In these cases, everything else should be NA except what the policy was
universal <- df %>%
	select(contains("Answer.")) %>%
	filter(Answer.universal == "yes")

# All instances where universal was yes but something else was filled in
universalWrong <- universal %>%
	filter(!is.na(Answer.optinpolicy))

# All instances where universal was marked no
# In these cases, everything else should be answered except universal policy
notUniversal <- df %>%
	select(contains("Answer.")) %>%
	filter(Answer.universal == "no")

notUniversalWrong <- notUniversal %>%
	filter(is.na(Answer.optinpolicy) | !is.na(Answer.universalpolicy))

# Lengths of the above
universalC <- nrow(universal)
universalWrongC <- nrow(universalWrong)
notUniversalC <- nrow(notUniversal)
notUniversalWrongC <- nrow(notUniversalWrong)
```

In total, there were `r universalC` surveys where universal was marked as yes, and `r universalWrongC` of these did not follow the logical flow. Of the `r notUniversalC` surveys where universal was marked no, `r notUniversalWrongC` did not follow the logical flow of the survey.

Given as percentages, `r round(universalWrongC / universalC, 2)` percent of those that were marked yes were done wrong, and `r round(notUniversalWrongC / notUniversalC, 2)` percent of answers marked no were done wrong. In total, `r round((universalWrongC + notUniversalWrongC) / (universalC + notUniversalC), 2)` percent of survey responses where the answerer could determine whether or not the policy was universal did not follow the logical flow of the survey.


# Discussion and Recommendations

We are going to want both a time-based cutoff and a content-based test to detect workers who are not in fact seeking out the information they are prompted for. These could be based on the logic of the question flows, for example. On the other hand, even workers who got good answers have also occasionally violated the survey logic. 

Possibly the word "universal" is confusing, but possibly there's also issues with the conditional flow of the survey. 

Possible: confusion between pass/fail and pass/no credit. 

- Elisabeth: the reason we care is that pass/no credit doesn't affect GPA, unlike pass/fail. Pass/no credit is usually explicit that the results won't go into GPA, whereas pass/fail is often less explicit. So this is often not explicitly stated. Lots of schools do things like: "satisfactory/not satisfactory" which can be very difficult for a worker to calculate. Suggestion: third category, "pass/not pass," GPA impact unclear. Maybe a new question: letter-grading scheme or no? 

- Josef: they don't seem to GET the logical flow of the survey. 


