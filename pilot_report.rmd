---
title: "Pilot Study Report"
author: "Elisabeth, Josef, and Phil"
date: "7/29/2020"
output:
  html_document:
    theme: united
    toc: true
    toc_depth: 1
    toc_float: true
    number_sections: true
---

# Purpose

In this report, we will study the quality of the answers we received from Amazon Mechanical Turk crowdsourcers in our pilot study. Our primary aims are to: 

1. Assess the **quality** of the answers we received. If we ran exactly the same methodology on the full list of institutions, how much reliable data would we gather? 
2. **Make recommendations** for improving the quality of the data we gather in the large-scale study. 

# Approach {.tabset}

## Data Completeness

A basic question: how often do we have any data at all? Possible metrics: 

1. How many `NA`s do we have?
2. For how many institutions do we have at least one answer in each question?

## Presence of Consensus

If we do have data, how often is there a clear consensus on the answer? Some possible metrics: 

1. In which schools and questions is there full consensus among the workers?
2. In which schools and questions is there a clear majority answer? 

## Reliability of Answers

We've noticed at least one case in which it appears that a worker did not even attempt to retrieve the information, and instead just clicked through the form. Can we detect this kind of behavior and discourage it? Or, at least, can we discount those answers in the analysis phase? 
This is a future question we might take up -- does not need to be pursued in this early report. Some possible diagnostics: 

1. Answer-level consistency checks. Did a worker give contradictory answers to two questions for the same school?
2. Worker-level assessments. Worker rating; average time spent on task; comparing answers across multiple schools. 

# Analysis

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(knitr)

df <- read_csv("data/Batch_4133302_batch_results.csv")
```

Here are a few things that I looked at very briefly -- feel free to modify, discard, or use. 

## Data Completeness
The data has ```r nrow(df)``` rows. 

```{r}
sub <- df %>% 
	select(LifetimeApprovalRate, AcceptTime, SubmitTime, Input.INSTNM,  contains("Answer"))
```

```{r, warning=FALSE, message=FALSE}

completeness <- sub %>% 
	group_by(Input.INSTNM) %>% 
	summarise(across(contains("Answer"), ~mean(!is.na(.x)))) %>% 
	summarise(across(contains("Answer"), mean)) %>% 
	pivot_longer(contains("Answer")) %>% 
	rename(`% Complete` = value)

num_answers <- sub %>% 
	group_by(Input.INSTNM) %>% 
	summarise(across(contains("Answer"), 
									 ~length(unique(na.omit(.x))))) %>% 
	summarise(across(contains("Answer"), mean)) %>% 
	pivot_longer(contains("Answer")) %>% 
	rename(`Mean # Distinct Answers` = value)

completeness %>% 
	left_join(num_answers, by = c("name" = "name")) %>% 
	rename(Question = name) %>% 
	arrange(desc(`% Complete`)) %>% 
	kable(digits = c(0, 2, 1))
```

To relate this data back to the survey, the survey questions and results are paraphrased below.

- **Q1: Is there an universal policy?** If yes, answer Q2, if no, skip to Q3.
We always have an answer for whether the policy was universal, which is good.

- **Q2: What was the universal policy?**
We have a specified universal policy 52% of the time. If the survey had been completed correctly, then we would expect 48% completeness on Q3-Q6.

- **Q3: If there was no universal policy, what was the default policy?**
We have 72% completeness on this question, indicating that the workers are misunderstanding the logical flow of the survey.

- **Q4: What was the opt-in policy ?**
As the questions get more complex, completeness drops to 69%.

- **Q5: How was the opt-in policy applied?**
Completeness remains at 69%. This makes sense becuase typically the opt-in policy and it's application are described together on websites.

- **Q6: When was the deadline for choosing the opt-in policy? (before or after grades)**
Completeness remains at 69%. This seems unlikely because it's a harder piece of information to find, unless we're counting "Cannot Determine" as a complete answer?

Questions 7 and 8 track worker confidence and comments, so their completeness is not very informative.

## Majority

How often is there a majority answer to a given question? In this quick analysis, we define "majority" as: 

1. At least two workers agree on a question. 
2. Any other workers have picked different answer: there is no other answer with two workers endorsing it. 

```{r, warning=FALSE, message=FALSE}

df %>% 
	select(Input.INSTNM, contains("Answer")) %>% 
	mutate(across(contains("Answer"), as.character)) %>% 
	pivot_longer(contains("Answer")) %>% 
	filter(!is.na(value), value != "cannotdetermine") %>%  # treating NA and "cannotdetermine" as the same
	group_by(Input.INSTNM, name, value) %>% 
	summarise(n = n()) %>% 
	mutate(majority_exists = (max(n) > 1) &  ((max(n) / sum(n)) > .5)) %>% 
	summarise(majority_exists = max(majority_exists)) %>% 
	ungroup() %>% 
	group_by(name) %>%
	summarise(`% Majority Exists` = sum(majority_exists)/length(unique(df$Input.INSTNM))) %>% 
	arrange(desc(`% Majority Exists`)) %>% 
	kable(digits = 2)
```

- We have the most majority consensus on Q1, which also has the most completeness. This could be promising, or it could just be because Q1 is binary.

- The three questions regarding an opt-in policy (default policy, opt-in policy, application of opt-in policy) all had consensus about half the time. The deadline had consensus somewhat less than half the time.

- The very low consensus on the universal policy is disturbing, and likely indicates worker confusion.

- **Further Thought** Are we counting it as consensus when at least 2 workers respond that they cannot determine? 

## Reliability of Answers

### Worker Reliability Based on Time Spent

Can we find suspicious workers based on their work time? The histogram below shows the distribution of work times. 

```{r}
df %>% 
	ggplot() + 
	aes(x = WorkTimeInSeconds) + 
	geom_histogram(bins = 100)
```

We can retrieve a list of workers who completed the survey in less than the minimum time in both of the following ways:

```{r, warning=FALSE, message=FALSE}
# Elisabeth's first try

min_time <- quantile(df$WorkTimeInSeconds, 0.25)
sus_workers <- c()

for(i in 1:nrow(df))
  {if(df$WorkTimeInSeconds[i] < min_time)
  {sus_workers[i] <- df$WorkerId[i]}}

unique(sus_workers)
```

```{r, warning = FALSE, message = FALSE}
# A more idiomatic to R version from Phil

# x %>% f(y) is the same as f(x,y)
df %>% 
	mutate(sus = WorkTimeInSeconds < min_time) %>% # boolean column for suspicious workers
	filter(sus) %>%                                # keep only those rows for which sus = TRUE
	distinct(WorkerId) %>%                         # show each distinct ID once
	kable()                                        # print it all pretty-like
```

- **Follow-up questions**: How many total answers would we lose if we discarded the work of suspicious workers? Also, is there a more systematic way to choose the quantile? 

The first priority here is determining how to choose the quantile, and whether we want to choose a quantile or a specific minimum time. Ideally, the histogram would give us an idea of where the minimum should be, but it's somewhat unclear. There is a spike around 45 seconds that could potentially be a good cut off time. Possibly we could identify some workers whose answers seem unreliable, and use their work times as references?


### Logical Flow

Could and did the workers follow the logical flow of the questions? The logical flow was based on whether or not there was a universal policy, so we have two cases.

1. The worker marked "yes" for universal. In this case, the worker did not follow the logical flow if they answered any of the questions besides what the universal policy was.
2. The worker marked "no" for universal. In this case, the worker did not follow the logical flow if they left blank any of the questions besides universal policy or if they answered universal policy.

```{r, warning=FALSE, message=FALSE, include=FALSE}
# All points for which universal was marked yes
# In these cases, everything else should be NA except what the policy was
universal <- df %>%
	select(contains("Answer.")) %>%
	filter(Answer.universal == "yes")

# All instances where universal was yes but something else was filled in
universalWrong <- universal %>%
	filter(!is.na(Answer.optinpolicy))

# All instances where universal was marked no
# In these cases, everything else should be answered except universal policy
notUniversal <- df %>%
	select(contains("Answer.")) %>%
	filter(Answer.universal == "no")

notUniversalWrong <- notUniversal %>%
	filter(is.na(Answer.optinpolicy) | !is.na(Answer.universalpolicy))

# Lengths of the above
universalC <- nrow(universal)
universalWrongC <- nrow(universalWrong)
notUniversalC <- nrow(notUniversal)
notUniversalWrongC <- nrow(notUniversalWrong)
```

In total, there were `r universalC` surveys where universal was marked as yes, and `r universalWrongC` of these did not follow the logical flow. Of the `r notUniversalC` surveys where universal was marked no, `r notUniversalWrongC` did not follow the logical flow of the survey.

Given as percentages, `r round(100 * universalWrongC / universalC, 2)` percent of those that were marked yes were done wrong, and `r round(100 * notUniversalWrongC / notUniversalC, 2)` percent of answers marked no were done wrong. In total, `r round(100 * (universalWrongC + notUniversalWrongC) / (universalC + notUniversalC), 2)` percent of survey responses where the answerer could determine whether or not the policy was universal did not follow the logical flow of the survey.


We also tested this the following way: 
```{r}
num_conflicts <- df %>% 
	group_by(Input.INSTNM, WorkerId) %>% 
	summarise(conflict = sum((Answer.universal == "yes")*(!is.na(Answer.optinpolicy)))) %>% 
	summarise(conflict = sum(conflict)) %>% 
	summarise(num_conflicts = sum(conflict, na.rm = T))

```
There were `r num_conflicts$num_conflicts[1]` instances in which workers answered the the policy was "universal" but proceeded to answer the opt-in policy question as well. 



# Discussion and Recommendations

We are going to want both a time-based cutoff and a content-based test to detect workers who are not in fact seeking out the information they are prompted for. These could be based on the logic of the question flows, for example. On the other hand, even workers who got good answers have also occasionally violated the survey logic. 

Possibly the word "universal" is confusing, but possibly there's also issues with the conditional flow of the survey. 

Possible: confusion between pass/fail and pass/no credit. 

- Elisabeth: the reason we care is that pass/no credit doesn't affect GPA, unlike pass/fail. Pass/no credit is usually explicit that the results won't go into GPA, whereas pass/fail is often less explicit. So this is often not explicitly stated. Lots of schools do things like: "satisfactory/not satisfactory" which can be very difficult for a worker to calculate. Suggestion: third category, "pass/not pass," GPA impact unclear. Maybe a new question: letter-grading scheme or no? 

- Josef: they don't seem to GET the logical flow of the survey. 


