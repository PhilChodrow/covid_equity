---
title: "Pilot Study Report"
author: "Elisabeth, Josef, and Phil"
date: "7/29/2020"
output:
  html_document:
    theme: united
    toc: true
    toc_depth: 1
    toc_float: true
    number_sections: true
---

# Purpose

In this report, we will study the quality of the answers we received from Amazon Mechanical Turk crowdsourcers in our pilot study. Our primary aims are to: 

1. Assess the **quality** of the answers we received. If we ran exactly the same methodology on the full list of institutions, how much reliable data would we gather? 
2. **Make recommendations** for improving the quality of the data we gather in the large-scale study. 

<!-- # Approach {.tabset} -->

<!-- ## Data Completeness -->

<!-- A basic question: how often do we have any data at all? Possible metrics:  -->

<!-- 1. How many `NA`s do we have? -->
<!-- 2. For how many institutions do we have at least one answer in each question? -->

<!-- ## Presence of Consensus -->

<!-- If we do have data, how often is there a clear consensus on the answer? Some possible metrics:  -->

<!-- 1. In which schools and questions is there full consensus among the workers? -->
<!-- 2. In which schools and questions is there a clear majority answer?  -->

<!-- ## Reliability of Answers -->

<!-- We've noticed at least one case in which it appears that a worker did not even attempt to retrieve the information, and instead just clicked through the form. Can we detect this kind of behavior and discourage it? Or, at least, can we discount those answers in the analysis phase?  -->
<!-- This is a future question we might take up -- does not need to be pursued in this early report. Some possible diagnostics:  -->

<!-- 1. Answer-level consistency checks. Did a worker give contradictory answers to two questions for the same school? -->
<!-- 2. Worker-level assessments. Worker rating; average time spent on task; comparing answers across multiple schools.  -->

# Paraphrase of Survey

For reference, we loosely paraphrase the survey questions here. 

- Q1: Is there an universal policy? If yes, answer Q2, if no, skip to Q3.

- Q2: What was the universal policy?

- Q3: If there was no universal policy, what was the default policy?

- Q4: What was the opt-in policy ?

- Q5: How was the opt-in policy applied?

- Q6: When was the deadline for choosing the opt-in policy? (before or after grades)


# Analysis {.tabset}

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(knitr)

df <- read_csv("data/Batch_4133302_batch_results.csv")

# This next code will access the second round of pilot data
df2 <- read_csv("data/Batch_4149595_batch_results.csv")


df <- df2 # for compatibility, comment out if needed

```

For the analysis, `df` is the data frame containing the pilot data. 

## Data Completeness

For a first, simple check, we study how frequently an answer is provided to each question, as well as the mean number of distinct answers. In this analysis, we include "cannotdetermine" as a valid answer. 

We also study the mean number of distinct answers provided to each question. 

```{r, warning=FALSE, message=FALSE}
sub <- df %>% 
	select(LifetimeApprovalRate, AcceptTime, SubmitTime, Input.INSTNM,  contains("Answer"))

completeness <- sub %>% 
	group_by(Input.INSTNM) %>% 
	summarise(across(contains("Answer"), ~mean(!is.na(.x)))) %>% 
	summarise(across(contains("Answer"), mean)) %>% 
	pivot_longer(contains("Answer")) %>% 
	rename(`% Complete` = value)

num_answers <- sub %>% 
	group_by(Input.INSTNM) %>% 
	summarise(across(contains("Answer"), 
									 ~length(unique(na.omit(.x))))) %>% 
	summarise(across(contains("Answer"), mean)) %>% 
	pivot_longer(contains("Answer")) %>% 
	rename(`Mean # Distinct Answers` = value)

completeness %>% 
	left_join(num_answers, by = c("name" = "name")) %>% 
	rename(Question = name) %>% 
	arrange(desc(`% Complete`)) %>% 
	kable(digits = c(0, 2, 1))
```

We always have an answer for whether the policy was universal, which is good.
We have a specified universal policy 52% of the time. If the survey had been completed correctly, then we would expect 48% completeness on Q3-Q6, which are about the opt-in policy. Instead, we have 72% completeness on Q3, indicating that the workers are misunderstanding the logical flow of the survey. As the questions get more complex, completeness drops to 69% for Q4-6. The consistent 69% completeness makes sense for Q4 and Q5 becuase typically the opt-in policy and its application are described together on most websites. However, it's suspicious that the completeness remains at 69% for Q6, becuase typically the deadline for choosing a grading policy is harder to find. Note that Q7 and Q8 track worker confidence and comments, so their completeness is not very informative.


## Presence of Majority Opinion

How often is there a majority answer to a given question? This is important to track, since we will generally use this as a guide to the correct answer when possible. 

For the purpose of this analysis, we define "majority" as: 

1. At least two workers agree on a question. 
2. Any other workers have picked different answer: there is no other answer with two workers endorsing it. 

```{r}

sub <- df %>% 
	select(Input.INSTNM, contains("Answer")) %>% 
	mutate(across(contains("Answer"), as.character)) %>% 
	group_by(Input.INSTNM) %>% 
	mutate(universal_no = sum(Answer.universal == "yes"),
				 universal_yes = sum(Answer.universal == "no"),
				 universal_cd = sum(Answer.universal == "cannotdetermine"),
				 universal_policy = case_when(
				 	(universal_yes > 1) && (universal_no < 2) ~ "yes",
				 	(universal_no > 1) && (universal_yes < 2) ~ "no",
				 	TRUE ~ "unknown"
				 )) %>% 
	select(-universal_no, universal_yes, universal_cd)

sub <- sub %>% 
	pivot_longer(contains("Answer"))  
```

```{r}

sub %>% 
	filter(!is.na(value), value != "unknown") %>%
	group_by(Input.INSTNM, name, value, universal_policy) %>% 
	summarise(n = n()) %>% 
	mutate(majority_exists = (max(n) > 1) &  ((max(n) / sum(n)) > 0.5)) %>% 
	group_by(Input.INSTNM, name, universal_policy) %>% 
	summarise(majority_exists = max(majority_exists)) %>% 
	ungroup() %>% 
	group_by(name, universal_policy) %>%
	summarise(`% Majority Exists` = sum(majority_exists)) %>% 
	pivot_wider(names_from = universal_policy, values_from = `% Majority Exists`) %>% 
	select(Question = name, universal = yes, not_universal = no, unknown) %>% 
	kable(digits = 2)
	
```

```{r}

sub %>% 
	group_by(universal_policy) %>% 
	summarise(denom = n_distinct(Input.INSTNM)) %>% 
	pivot_wider(names_from = universal_policy, values_from = denom) %>%
	select(universal = yes, not_universal = no, unknown) %>% 
	kable()

```

So, big picture: 

- We lose about 20% to workers not being able to identify whether a policy is universal. 
- Of the universal institutions, we can get information on the policy for maybe half of them. 
- Of the non-universal institutions, we can get information about the choices for most (~75%) of them. 

In total, we can probably reasonably expect to get useful information of some nature for 60-70% of schools. This is all assuming that two out of four concurring workers is sufficient to count answers.  


## Time Spent

Can we find suspicious workers based on their work time? The histogram below shows the distribution of work times. 

```{r}
df %>% 
	ggplot() + 
	aes(x = WorkTimeInSeconds) + 
	geom_histogram(bins = 100)
```

We can retrieve a list of workers who completed the survey in less than the minimum time as follows:

```{r, warning=FALSE, message=FALSE}

min_time <- quantile(df$WorkTimeInSeconds, 0.25)
sus_workers <- c()

for(i in 1:nrow(df))
  {if(df$WorkTimeInSeconds[i] < min_time)
  {sus_workers[i] <- df$WorkerId[i]}}

unique(sus_workers)
```

<!-- ```{r, warning = FALSE, message = FALSE} -->
<!-- # A more idiomatic to R version from Phil -->

<!-- # x %>% f(y) is the same as f(x,y) -->
<!-- df %>%  -->
<!-- 	mutate(sus = WorkTimeInSeconds < min_time) %>% # boolean column for suspicious workers -->
<!-- 	filter(sus) %>%                                # keep only those rows for which sus = TRUE -->
<!-- 	distinct(WorkerId) %>%                         # show each distinct ID once -->
<!-- 	kable()                                        # print it all pretty-like -->
<!-- ``` -->

<!-- - **Follow-up questions**: How many total answers would we lose if we discarded the work of suspicious workers? Also, is there a more systematic way to choose the quantile?  -->

The first priority here is determining how to choose the quantile, and whether we want to choose a quantile or a specific minimum time. Ideally, the histogram would give us an idea of where the minimum should be, but it's somewhat unclear. There is a spike around 45 seconds that could potentially be a good cut off time. Possibly we could identify some workers whose answers seem unreliable, and use their work times as references?


## Control Flow Violations

Could and did the workers follow the logical flow of the questions? The logical flow was based on whether or not there was a universal policy, so we have two cases:

1. The worker marked "yes" for universal. In this case, the worker did not follow the logical flow if they answered any of the questions besides what the universal policy was.
2. The worker marked "no" for universal. In this case, the worker did not follow the logical flow if they left blank any of the questions besides universal policy or if they answered universal policy.

```{r, warning=FALSE, message=FALSE, include=FALSE}
# All points for which universal was marked yes
# In these cases, everything else should be NA except what the policy was
universal <- df %>%
	select(contains("Answer.")) %>%
	filter(Answer.universal == "no")

# All instances where universal was yes but something else was filled in
universalWrong <- universal %>%
	filter(!is.na(Answer.optinpolicy))

# All instances where universal was marked no
# In these cases, everything else should be answered except universal policy
notUniversal <- df %>%
	select(contains("Answer.")) %>%
	filter(Answer.universal == "yes")

notUniversalWrong <- notUniversal %>%
	filter(is.na(Answer.optinpolicy) | !is.na(Answer.universalpolicy))

# Lengths of the above
universalC1 <- nrow(universal)
universalWrongC1 <- nrow(universalWrong)
notUniversalC1 <- nrow(notUniversal)
notUniversalWrongC1 <- nrow(notUniversalWrong)
```

```{r, warning=FALSE, message=FALSE}
# All points for which universal was marked yes (Means there was NOT a universal policy)
# In these cases, universal policy should be blank
universal <- df2 %>%
	select(contains("Answer.")) %>%
	filter(Answer.universal == "yes")

# All instances where universal was yes but universal policy was filled in
universalWrong <- universal %>%
	filter(!is.na(Answer.universalpolicy))

# All instances where universal was marked no (Means there was a universal policy)
# In these cases, nothing else should be answered except universal policy
notUniversal <- df2 %>%
	select(contains("Answer.")) %>%
	filter(Answer.universal == "no")

notUniversalWrong <- notUniversal %>%
	filter(!is.na(Answer.optinpolicy) | is.na(Answer.universalpolicy))
```

```{r, warning=FALSE, message=FALSE, include=FALSE}
# Lengths of the above
universalC <- nrow(universal)
universalWrongC <- nrow(universalWrong)
notUniversalC <- nrow(notUniversal)
notUniversalWrongC <- nrow(notUniversalWrong)
```




In total, there were `r universalC` surveys where universal was marked as yes, and `r universalWrongC` of these did not follow the logical flow. Of the `r notUniversalC` surveys where universal was marked no, `r notUniversalWrongC` did not follow the logical flow of the survey.

Given as percentages, `r round(100 * universalWrongC / universalC, 2)` percent of those that were marked yes were done wrong, and `r round(100 * notUniversalWrongC / notUniversalC, 2)` percent of answers marked no were done wrong. In total, `r round(100 * (universalWrongC + notUniversalWrongC) / (universalC + notUniversalC), 2)` percent of survey responses where the answerer could determine whether or not the policy was universal did not follow the logical flow of the survey.

This is only a slight improvement from before when `r round(100 * (universalWrongC1 + notUniversalWrongC1) / (universalC1 + notUniversalC1), 2)` percent of survey answers failed to follow the logical flow.

<!-- This next part still needs to be updated, I beleive it's Phil's code and I'm not fully sure how it works-->
We also tested this the following way: 
```{r, warning=FALSE, message=FALSE}
num_conflicts <- df %>% 
	group_by(Input.INSTNM, WorkerId) %>% 
	summarise(conflict = sum((Answer.universal == "yes")*(!is.na(Answer.optinpolicy)))) %>% 
	summarise(conflict = sum(conflict)) %>% 
	summarise(num_conflicts = sum(conflict, na.rm = T))

```
There were `r num_conflicts$num_conflicts[1]` instances in which workers answered the the policy was "universal" but proceeded to answer the opt-in policy question as well. 



# Discussion and Recommendations

We are going to want both a time-based cutoff and a content-based test to detect workers who are not in fact seeking out the information they are prompted for. These could be based on the logic of the question flows, for example. On the other hand, even workers who got good answers have also occasionally violated the survey logic. 

Possibly the word "universal" is confusing, but possibly there's also issues with the conditional flow of the survey. 

We also noted that there may be some confusion on the difference between pass/fail and pass/no credit policies. We may want to make sure it is clear that pass/no credit is a policy that does not affect GPA whereas a fail affects GPA. However, some institutions do not make it clear whether or not the policy will affect GPA, in which case, we have a few options to try to make the survey easier to fill out:

- Add a new grading policy option for pass/not pass specifying that the impact on GPA is unclear.

- Add a new question that is only used to determine whether or not there is a letter grading scheme.

The quantity of responses not following the logical flow of the survey would also indicate that some amount of the workers did not understand the logical flow. While some may have just not noticed it or ignored it, with `r round(100 * (universalWrongC + notUniversalWrongC) / (universalC + notUniversalC), 2)` percent not following the instructions, it seems likely something was unclear.


## Recommmendations for Survey Modifications

### Logical flow
	
This is what we previously had for improving the logical flow:

Can we reduce the number of questions they have to skip? It's possible this would make the survey easier to follow.

If we do manage to simplify the survey in this way, it might make the logical flow easier to understand. However, it is important to note that the logical flow can also be used as a quality check. If a small number of workers do not follow the instructions, we can guess that they were not paying much attention when answering the questions. Do we need a balance between simplifying the survey and using the flow as a quality check?

For example, if there were some way to automatically make the survey follow the logical flow (only show certain questions depending on the answers to other questions), we can guarantee no one is confused, but we also can no longer tell who was paying close attention when taking the survey.

After the changes to the survey, there was a slight improvement to the workers' ability to follow the logical flow, however not nearly an improvement to the extend we desired.
	
### Universal Policy Language

Definition of universal language policy is tripping people up. It seems likely that many workers are not really understanding the word "universal." The first time Elisabeth was reading the survey, she wasn't sure what it meant. We could possibly instead ask about whether there was any change to the grading policy. We could also try using other language such as a "default" grading policy, but there is a chance that turns out just as confusing.

The language of an "opt-in" policy seems a lot more clear. This means we might be able to flip the questions, and instead of asking about a universal policy, we ask if there was an optional policy. This avoids the whole issue of clearly defining what "universal" means in the context of the survey.

Since universal policies appear to be more rare, we can make the affirmative option correspond to the more common possibility. 

An example of a new question could be: "Did students get to make a choice about how they were graded?"

### Implausible answers

Some of the answers seem to be pulled out of thin air. In some cases, none of the links have answers and they were not found by hand, but the workers managed to find them in a very small amount of time.

Detection of these seemingly fake answers might be possible, but this may just be an aspect of what workers we get based on settings, payment, and other worker options on Mechanical Turk.

Would it be possible to have the workers click on a link before answering the questions? This could encourage them to look at some pages before just answering randomly. We could also add a question asking which link the information was found on, but that could just be filled out randomly as well.

Making the survey less confusing might help to reduce fabricated information, as workers will have to put in less effort to answer the questions. It is also possible that reducing the number of questions would encourage workers to look harder for the answers, as it would be less work.